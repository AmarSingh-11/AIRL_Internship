{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9753c33-3ef1-41ae-86cd-7a5e96283107",
   "metadata": {},
   "source": [
    "# Vision Transformer on CIFAR-10 (PyTorch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a0b3d-5f47-4a10-9f24-e1a398e0acf4",
   "metadata": {},
   "source": [
    "# Setup and Advanced Training Utilities for ViT\n",
    "\n",
    "# This section handles the necessary imports, hardware setup, reproducibility settings, and introduces three critical utility modules (**DropPath, WarmupCosine Scheduler, Label Smoothing**) required for robust Vision Transformer (ViT) training, particularly on smaller datasets like CIFAR-10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858f5d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:59:06.152524Z",
     "iopub.status.busy": "2025-10-01T13:59:06.152030Z",
     "iopub.status.idle": "2025-10-01T13:59:16.338507Z",
     "shell.execute_reply": "2025-10-01T13:59:16.337123Z"
    },
    "id": "ayStcqH8DwDh",
    "outputId": "1cf1ecb6-2777-4bb4-da2d-b161a4a4ff84",
    "papermill": {
     "duration": 10.193674,
     "end_time": "2025-10-01T13:59:16.340695",
     "exception": false,
     "start_time": "2025-10-01T13:59:06.147021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla T4 (UUID: GPU-47c9d731-ee0a-8b3e-5aca-0f092b4b3042)\r\n",
      "GPU 1: Tesla T4 (UUID: GPU-1a08dde5-652d-14c6-0fdd-8f08473c8822)\r\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L\n",
    "\n",
    "import math, os, random, json, time\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Repro\n",
    "seed = 1337\n",
    "random.seed(seed); np.random.seed(seed)\n",
    "torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- DropPath (stochastic depth) ---\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, p=0.0):\n",
    "        super().__init__(); self.p = float(p)\n",
    "    def forward(self, x):\n",
    "        if self.p == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep = 1.0 - self.p\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        mask = x.new_empty(shape).bernoulli_(keep).div_(keep)\n",
    "        return x * mask\n",
    "\n",
    "# --- Warmup + Cosine scheduler (step every batch) ---\n",
    "class WarmupCosine:\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, min_lr=1e-5):\n",
    "        self.opt = optimizer\n",
    "        self.warm = int(warmup_steps)\n",
    "        self.total = int(total_steps)\n",
    "        self.min_lr = float(min_lr)\n",
    "        self.base = [g['lr'] for g in optimizer.param_groups]\n",
    "        self.t = 0\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, g in enumerate(self.opt.param_groups):\n",
    "            base = self.base[i]\n",
    "            if self.t <= self.warm:\n",
    "                lr = base * self.t / max(1, self.warm)\n",
    "            else:\n",
    "                p = (self.t - self.warm) / max(1, self.total - self.warm)\n",
    "                lr = self.min_lr + 0.5*(base - self.min_lr)*(1 + math.cos(math.pi * p))\n",
    "            g['lr'] = lr\n",
    "    def get_last_lr(self):\n",
    "        return [g['lr'] for g in self.opt.param_groups]\n",
    "\n",
    "# --- Label smoothing CE (works well for ViT) ---\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, eps=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, logits, target):\n",
    "        n = logits.size(-1)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logits).fill_(self.eps/(n-1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.eps)\n",
    "        return torch.mean(torch.sum(-true_dist * logp, dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12277c38-7181-4737-8ab1-b6b6b315a18d",
   "metadata": {},
   "source": [
    "# ==== ViT implementation ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "badf2a62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:59:16.349539Z",
     "iopub.status.busy": "2025-10-01T13:59:16.349115Z",
     "iopub.status.idle": "2025-10-01T13:59:16.377262Z",
     "shell.execute_reply": "2025-10-01T13:59:16.375739Z"
    },
    "id": "MQF9eXtwfl8y",
    "papermill": {
     "duration": 0.035008,
     "end_time": "2025-10-01T13:59:16.379000",
     "exception": false,
     "start_time": "2025-10-01T13:59:16.343992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==== ViT implementation ====\n",
    "from torch import nn\n",
    "\n",
    "class NewGELUActivation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        assert self.image_size % self.patch_size == 0\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size,\n",
    "                                    kernel_size=self.patch_size, stride=self.patch_size)\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)                    # (B, D, H/ps, W/ps)\n",
    "        x = x.flatten(2).transpose(1, 2)          # (B, N, D)\n",
    "        return x\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config[\"hidden_size\"]))\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.zeros(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"])\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)              # (B, N, D)\n",
    "        B = x.size(0)\n",
    "        cls = self.cls_token.expand(B, -1, -1)    # (B,1,D)\n",
    "        x = torch.cat([cls, x], dim=1)            # prepend CLS\n",
    "        x = x + self.position_embeddings          # learnable pos\n",
    "        return self.dropout(x)\n",
    "\n",
    "class FasterMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_heads = config[\"num_attention_heads\"]\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.qkv = nn.Linear(self.hidden_size, self.hidden_size * 3, bias=config[\"qkv_bias\"])\n",
    "        self.attn_drop = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        self.proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.out_drop = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = [t.view(B, N, self.num_heads, self.head_dim).transpose(1, 2) for t in qkv]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        out = attn @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, N, C)\n",
    "        out = self.out_drop(self.proj(out))\n",
    "        if output_attentions:\n",
    "            return out, attn\n",
    "        return out, None\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.act = NewGELUActivation()\n",
    "        self.fc2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.drop = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.act(self.fc1(x)))\n",
    "        x = self.drop(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.attn = FasterMultiHeadAttention(config) if config.get(\"use_faster_attention\", True) else MultiHeadAttention(config)\n",
    "        self.drop_path1 = DropPath(drop_path)\n",
    "        self.norm2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.drop_path2 = DropPath(drop_path)\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        attn_out, attn_probs = self.attn(self.norm1(x), output_attentions=output_attentions)\n",
    "        x = x + self.drop_path1(attn_out)          # residual\n",
    "        mlp_out = self.mlp(self.norm2(x))\n",
    "        x = x + self.drop_path2(mlp_out)           # residual\n",
    "        if output_attentions: return x, attn_probs\n",
    "        return x, None\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        depth = config[\"num_hidden_layers\"]\n",
    "        # stochastic depth schedule (0 -> drop_path)\n",
    "        dpr = torch.linspace(0, config.get(\"drop_path\", 0.1), steps=depth).tolist()\n",
    "        self.blocks = nn.ModuleList([Block(config, drop_path=dpr[i]) for i in range(depth)])\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        atts = []\n",
    "        for blk in self.blocks:\n",
    "            x, att = blk(x, output_attentions=output_attentions)\n",
    "            if output_attentions: atts.append(att)\n",
    "        if output_attentions: return x, atts\n",
    "        return x, None\n",
    "\n",
    "class ViTForClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = Embeddings(config)\n",
    "        self.encoder = Encoder(config)\n",
    "        self.norm = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.head = nn.Linear(config[\"hidden_size\"], config[\"num_classes\"])\n",
    "        self.apply(self._init_weights)\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        x = self.embedding(x)\n",
    "        x, atts = self.encoder(x, output_attentions=output_attentions)\n",
    "        cls = self.norm(x)[:, 0]                   # classify from CLS\n",
    "        logits = self.head(cls)\n",
    "        if output_attentions: return logits, atts\n",
    "        return logits, None\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.trunc_normal_(m.weight, std=self.config[\"initializer_range\"])\n",
    "            if getattr(m, \"bias\", None) is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, Embeddings):\n",
    "            nn.init.trunc_normal_(m.position_embeddings, std=self.config[\"initializer_range\"])\n",
    "            nn.init.trunc_normal_(m.cls_token, std=self.config[\"initializer_range\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd719e-61cc-4211-a607-030c921f0de8",
   "metadata": {},
   "source": [
    "# ==== Data: CIFAR-10 with strong aug ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65c792e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:59:16.387438Z",
     "iopub.status.busy": "2025-10-01T13:59:16.387203Z",
     "iopub.status.idle": "2025-10-01T13:59:22.093685Z",
     "shell.execute_reply": "2025-10-01T13:59:22.092740Z"
    },
    "id": "X9-qkH61fsHk",
    "papermill": {
     "duration": 5.712379,
     "end_time": "2025-10-01T13:59:22.095323",
     "exception": false,
     "start_time": "2025-10-01T13:59:16.382944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:02<00:00, 78.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== Data: CIFAR-10 with strong aug ====\n",
    "# CIFAR-10 stats\n",
    "MEAN, STD = (0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root='/content/data', train=True, download=True, transform=train_tfms)\n",
    "test_ds  = datasets.CIFAR10(root='/content/data', train=False, download=True, transform=test_tfms)\n",
    "\n",
    "# T4 safe batch sizes (AMP helps). Try 128 first; bump to 256 if memory is comfy.\n",
    "train_bs = 128\n",
    "test_bs  = 256\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=train_bs, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=test_bs, shuffle=False, num_workers=2, pin_memory=True)\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d47de-33b9-4264-b2a5-84d23dbf8822",
   "metadata": {},
   "source": [
    "# ==== Trainer with AMP + scheduler + smoothing ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9281d942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:59:22.104562Z",
     "iopub.status.busy": "2025-10-01T13:59:22.104305Z",
     "iopub.status.idle": "2025-10-01T13:59:22.116357Z",
     "shell.execute_reply": "2025-10-01T13:59:22.115520Z"
    },
    "id": "tuRcm9Xpfuri",
    "papermill": {
     "duration": 0.018067,
     "end_time": "2025-10-01T13:59:22.117683",
     "exception": false,
     "start_time": "2025-10-01T13:59:22.099616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==== Trainer with AMP + scheduler + smoothing ====\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, loss_fn, device, scheduler=None, exp_name=\"vit-exp\"):\n",
    "        self.model = model.to(device)\n",
    "        self.opt = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        self.sched = scheduler\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
    "        self.exp_name = exp_name\n",
    "        os.makedirs(f\"/content/{exp_name}\", exist_ok=True)\n",
    "\n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        total_loss, total, correct = 0.0, 0, 0\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "            self.opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(self.device==\"cuda\")):\n",
    "                logits, _ = self.model(imgs)\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.opt)\n",
    "            self.scaler.update()\n",
    "            if self.sched: self.sched.step()\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            total += imgs.size(0)\n",
    "            correct += (logits.argmax(1) == labels).sum().item()\n",
    "        return total_loss/total, correct/total\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        total_loss, total, correct = 0.0, 0, 0\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "            logits, _ = self.model(imgs)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            total += imgs.size(0)\n",
    "            correct += (logits.argmax(1) == labels).sum().item()\n",
    "        return total_loss/total, correct/total\n",
    "\n",
    "    def fit(self, train_loader, test_loader, epochs, save_best=True):\n",
    "        best_acc = 0.0\n",
    "        history = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "        for ep in range(1, epochs+1):\n",
    "            tr_loss, tr_acc = self.train_epoch(train_loader)\n",
    "            te_loss, te_acc = self.evaluate(test_loader)\n",
    "            history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "            history[\"test_loss\"].append(te_loss);  history[\"test_acc\"].append(te_acc)\n",
    "            print(f\"Epoch {ep:03d} | train_acc={tr_acc:.4f} loss={tr_loss:.4f} | test_acc={te_acc:.4f} loss={te_loss:.4f}\")\n",
    "            if save_best and te_acc > best_acc:\n",
    "                best_acc = te_acc\n",
    "                torch.save(self.model.state_dict(), f\"/content/{self.exp_name}/best.pth\")\n",
    "        print(\"Best test acc:\", best_acc)\n",
    "        with open(f\"/content/{self.exp_name}/metrics.json\", \"w\") as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b648ed9-74ce-4bde-90ee-10f88294600b",
   "metadata": {},
   "source": [
    "# ==== Config + train ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c6e3b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:59:22.125257Z",
     "iopub.status.busy": "2025-10-01T13:59:22.124817Z",
     "iopub.status.idle": "2025-10-01T17:18:34.136920Z",
     "shell.execute_reply": "2025-10-01T17:18:34.135940Z"
    },
    "id": "nB5NdwtcfxbH",
    "outputId": "2ff219e0-3a71-450f-d53d-b99ded8b7206",
    "papermill": {
     "duration": 11952.02578,
     "end_time": "2025-10-01T17:18:34.146824",
     "exception": false,
     "start_time": "2025-10-01T13:59:22.121044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1207295856.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
      "/tmp/ipykernel_19/1207295856.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(self.device==\"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_acc=0.2175 loss=2.1790 | test_acc=0.3073 loss=1.8423\n",
      "Epoch 002 | train_acc=0.3216 loss=1.9417 | test_acc=0.4051 loss=1.6341\n",
      "Epoch 003 | train_acc=0.3798 loss=1.8366 | test_acc=0.4682 loss=1.4862\n",
      "Epoch 004 | train_acc=0.4102 loss=1.7725 | test_acc=0.4658 loss=1.4663\n",
      "Epoch 005 | train_acc=0.4333 loss=1.7332 | test_acc=0.4935 loss=1.4099\n",
      "Epoch 006 | train_acc=0.4489 loss=1.6983 | test_acc=0.5256 loss=1.3226\n",
      "Epoch 007 | train_acc=0.4664 loss=1.6665 | test_acc=0.5539 loss=1.2643\n",
      "Epoch 008 | train_acc=0.4774 loss=1.6443 | test_acc=0.5541 loss=1.2623\n",
      "Epoch 009 | train_acc=0.4860 loss=1.6251 | test_acc=0.5650 loss=1.2471\n",
      "Epoch 010 | train_acc=0.4964 loss=1.6045 | test_acc=0.5652 loss=1.2348\n",
      "Epoch 011 | train_acc=0.5103 loss=1.5773 | test_acc=0.5644 loss=1.2059\n",
      "Epoch 012 | train_acc=0.5161 loss=1.5635 | test_acc=0.5746 loss=1.2144\n",
      "Epoch 013 | train_acc=0.5232 loss=1.5483 | test_acc=0.6008 loss=1.1183\n",
      "Epoch 014 | train_acc=0.5294 loss=1.5388 | test_acc=0.6076 loss=1.1192\n",
      "Epoch 015 | train_acc=0.5396 loss=1.5184 | test_acc=0.6034 loss=1.1265\n",
      "Epoch 016 | train_acc=0.5492 loss=1.4976 | test_acc=0.6237 loss=1.0739\n",
      "Epoch 017 | train_acc=0.5563 loss=1.4845 | test_acc=0.6150 loss=1.0873\n",
      "Epoch 018 | train_acc=0.5662 loss=1.4659 | test_acc=0.6505 loss=1.0153\n",
      "Epoch 019 | train_acc=0.5777 loss=1.4488 | test_acc=0.6530 loss=1.0216\n",
      "Epoch 020 | train_acc=0.5805 loss=1.4346 | test_acc=0.6622 loss=0.9766\n",
      "Epoch 021 | train_acc=0.5919 loss=1.4149 | test_acc=0.6721 loss=0.9496\n",
      "Epoch 022 | train_acc=0.6033 loss=1.3957 | test_acc=0.6590 loss=1.0080\n",
      "Epoch 023 | train_acc=0.6059 loss=1.3823 | test_acc=0.6790 loss=0.9348\n",
      "Epoch 024 | train_acc=0.6133 loss=1.3679 | test_acc=0.6893 loss=0.9289\n",
      "Epoch 025 | train_acc=0.6241 loss=1.3489 | test_acc=0.6995 loss=0.8818\n",
      "Epoch 026 | train_acc=0.6319 loss=1.3322 | test_acc=0.7054 loss=0.8837\n",
      "Epoch 027 | train_acc=0.6403 loss=1.3184 | test_acc=0.7180 loss=0.8412\n",
      "Epoch 028 | train_acc=0.6483 loss=1.3009 | test_acc=0.7103 loss=0.8533\n",
      "Epoch 029 | train_acc=0.6531 loss=1.2873 | test_acc=0.7114 loss=0.8486\n",
      "Epoch 030 | train_acc=0.6582 loss=1.2798 | test_acc=0.7352 loss=0.7930\n",
      "Epoch 031 | train_acc=0.6637 loss=1.2658 | test_acc=0.7370 loss=0.7919\n",
      "Epoch 032 | train_acc=0.6714 loss=1.2541 | test_acc=0.7336 loss=0.7997\n",
      "Epoch 033 | train_acc=0.6766 loss=1.2410 | test_acc=0.7495 loss=0.7710\n",
      "Epoch 034 | train_acc=0.6832 loss=1.2245 | test_acc=0.7407 loss=0.7713\n",
      "Epoch 035 | train_acc=0.6867 loss=1.2210 | test_acc=0.7465 loss=0.7681\n",
      "Epoch 036 | train_acc=0.6936 loss=1.2065 | test_acc=0.7636 loss=0.7230\n",
      "Epoch 037 | train_acc=0.6973 loss=1.1986 | test_acc=0.7704 loss=0.7017\n",
      "Epoch 038 | train_acc=0.7013 loss=1.1887 | test_acc=0.7682 loss=0.7113\n",
      "Epoch 039 | train_acc=0.7054 loss=1.1763 | test_acc=0.7688 loss=0.7034\n",
      "Epoch 040 | train_acc=0.7124 loss=1.1652 | test_acc=0.7821 loss=0.6770\n",
      "Epoch 041 | train_acc=0.7160 loss=1.1588 | test_acc=0.7804 loss=0.6849\n",
      "Epoch 042 | train_acc=0.7185 loss=1.1512 | test_acc=0.7800 loss=0.6807\n",
      "Epoch 043 | train_acc=0.7219 loss=1.1417 | test_acc=0.7854 loss=0.6659\n",
      "Epoch 044 | train_acc=0.7293 loss=1.1313 | test_acc=0.7785 loss=0.6858\n",
      "Epoch 045 | train_acc=0.7314 loss=1.1239 | test_acc=0.7916 loss=0.6499\n",
      "Epoch 046 | train_acc=0.7348 loss=1.1181 | test_acc=0.7908 loss=0.6523\n",
      "Epoch 047 | train_acc=0.7348 loss=1.1129 | test_acc=0.7923 loss=0.6380\n",
      "Epoch 048 | train_acc=0.7412 loss=1.1029 | test_acc=0.7953 loss=0.6423\n",
      "Epoch 049 | train_acc=0.7454 loss=1.0965 | test_acc=0.7933 loss=0.6423\n",
      "Epoch 050 | train_acc=0.7484 loss=1.0891 | test_acc=0.8061 loss=0.6114\n",
      "Epoch 051 | train_acc=0.7501 loss=1.0796 | test_acc=0.8102 loss=0.5979\n",
      "Epoch 052 | train_acc=0.7588 loss=1.0692 | test_acc=0.8056 loss=0.6106\n",
      "Epoch 053 | train_acc=0.7603 loss=1.0661 | test_acc=0.8125 loss=0.6007\n",
      "Epoch 054 | train_acc=0.7615 loss=1.0605 | test_acc=0.8044 loss=0.6179\n",
      "Epoch 055 | train_acc=0.7634 loss=1.0562 | test_acc=0.8103 loss=0.6038\n",
      "Epoch 056 | train_acc=0.7666 loss=1.0480 | test_acc=0.8097 loss=0.5909\n",
      "Epoch 057 | train_acc=0.7723 loss=1.0336 | test_acc=0.8124 loss=0.5905\n",
      "Epoch 058 | train_acc=0.7739 loss=1.0340 | test_acc=0.8091 loss=0.6000\n",
      "Epoch 059 | train_acc=0.7788 loss=1.0214 | test_acc=0.8221 loss=0.5693\n",
      "Epoch 060 | train_acc=0.7818 loss=1.0181 | test_acc=0.8157 loss=0.5855\n",
      "Epoch 061 | train_acc=0.7846 loss=1.0116 | test_acc=0.8214 loss=0.5711\n",
      "Epoch 062 | train_acc=0.7864 loss=1.0089 | test_acc=0.8217 loss=0.5671\n",
      "Epoch 063 | train_acc=0.7883 loss=1.0004 | test_acc=0.8225 loss=0.5576\n",
      "Epoch 064 | train_acc=0.7938 loss=0.9925 | test_acc=0.8269 loss=0.5565\n",
      "Epoch 065 | train_acc=0.7947 loss=0.9838 | test_acc=0.8194 loss=0.5749\n",
      "Epoch 066 | train_acc=0.7990 loss=0.9805 | test_acc=0.8306 loss=0.5490\n",
      "Epoch 067 | train_acc=0.8051 loss=0.9681 | test_acc=0.8269 loss=0.5536\n",
      "Epoch 068 | train_acc=0.8019 loss=0.9738 | test_acc=0.8297 loss=0.5529\n",
      "Epoch 069 | train_acc=0.8073 loss=0.9625 | test_acc=0.8287 loss=0.5437\n",
      "Epoch 070 | train_acc=0.8105 loss=0.9575 | test_acc=0.8286 loss=0.5486\n",
      "Epoch 071 | train_acc=0.8087 loss=0.9569 | test_acc=0.8312 loss=0.5370\n",
      "Epoch 072 | train_acc=0.8133 loss=0.9477 | test_acc=0.8290 loss=0.5456\n",
      "Epoch 073 | train_acc=0.8200 loss=0.9360 | test_acc=0.8412 loss=0.5154\n",
      "Epoch 074 | train_acc=0.8202 loss=0.9323 | test_acc=0.8363 loss=0.5303\n",
      "Epoch 075 | train_acc=0.8237 loss=0.9288 | test_acc=0.8423 loss=0.5109\n",
      "Epoch 076 | train_acc=0.8247 loss=0.9219 | test_acc=0.8377 loss=0.5344\n",
      "Epoch 077 | train_acc=0.8270 loss=0.9193 | test_acc=0.8427 loss=0.5233\n",
      "Epoch 078 | train_acc=0.8318 loss=0.9102 | test_acc=0.8386 loss=0.5274\n",
      "Epoch 079 | train_acc=0.8310 loss=0.9082 | test_acc=0.8458 loss=0.5106\n",
      "Epoch 080 | train_acc=0.8374 loss=0.8988 | test_acc=0.8435 loss=0.5097\n",
      "Epoch 081 | train_acc=0.8388 loss=0.8938 | test_acc=0.8485 loss=0.5013\n",
      "Epoch 082 | train_acc=0.8413 loss=0.8923 | test_acc=0.8486 loss=0.5044\n",
      "Epoch 083 | train_acc=0.8429 loss=0.8841 | test_acc=0.8516 loss=0.4994\n",
      "Epoch 084 | train_acc=0.8436 loss=0.8844 | test_acc=0.8522 loss=0.4958\n",
      "Epoch 085 | train_acc=0.8479 loss=0.8751 | test_acc=0.8467 loss=0.5061\n",
      "Epoch 086 | train_acc=0.8494 loss=0.8714 | test_acc=0.8502 loss=0.4972\n",
      "Epoch 087 | train_acc=0.8510 loss=0.8704 | test_acc=0.8454 loss=0.5164\n",
      "Epoch 088 | train_acc=0.8521 loss=0.8649 | test_acc=0.8469 loss=0.5094\n",
      "Epoch 089 | train_acc=0.8542 loss=0.8600 | test_acc=0.8543 loss=0.5009\n",
      "Epoch 090 | train_acc=0.8554 loss=0.8563 | test_acc=0.8523 loss=0.5010\n",
      "Epoch 091 | train_acc=0.8620 loss=0.8451 | test_acc=0.8557 loss=0.4941\n",
      "Epoch 092 | train_acc=0.8610 loss=0.8453 | test_acc=0.8536 loss=0.4894\n",
      "Epoch 093 | train_acc=0.8618 loss=0.8450 | test_acc=0.8593 loss=0.4834\n",
      "Epoch 094 | train_acc=0.8636 loss=0.8389 | test_acc=0.8604 loss=0.4805\n",
      "Epoch 095 | train_acc=0.8645 loss=0.8364 | test_acc=0.8587 loss=0.4813\n",
      "Epoch 096 | train_acc=0.8684 loss=0.8304 | test_acc=0.8568 loss=0.4859\n",
      "Epoch 097 | train_acc=0.8680 loss=0.8306 | test_acc=0.8588 loss=0.4804\n",
      "Epoch 098 | train_acc=0.8724 loss=0.8236 | test_acc=0.8571 loss=0.4937\n",
      "Epoch 099 | train_acc=0.8741 loss=0.8211 | test_acc=0.8568 loss=0.4841\n",
      "Epoch 100 | train_acc=0.8755 loss=0.8141 | test_acc=0.8584 loss=0.4844\n",
      "Epoch 101 | train_acc=0.8794 loss=0.8082 | test_acc=0.8609 loss=0.4804\n",
      "Epoch 102 | train_acc=0.8798 loss=0.8072 | test_acc=0.8649 loss=0.4682\n",
      "Epoch 103 | train_acc=0.8814 loss=0.8036 | test_acc=0.8560 loss=0.4956\n",
      "Epoch 104 | train_acc=0.8829 loss=0.7989 | test_acc=0.8625 loss=0.4730\n",
      "Epoch 105 | train_acc=0.8833 loss=0.7966 | test_acc=0.8605 loss=0.4819\n",
      "Epoch 106 | train_acc=0.8839 loss=0.7971 | test_acc=0.8597 loss=0.4802\n",
      "Epoch 107 | train_acc=0.8880 loss=0.7906 | test_acc=0.8642 loss=0.4802\n",
      "Epoch 108 | train_acc=0.8884 loss=0.7879 | test_acc=0.8663 loss=0.4689\n",
      "Epoch 109 | train_acc=0.8887 loss=0.7869 | test_acc=0.8636 loss=0.4824\n",
      "Epoch 110 | train_acc=0.8922 loss=0.7828 | test_acc=0.8656 loss=0.4735\n",
      "Epoch 111 | train_acc=0.8947 loss=0.7755 | test_acc=0.8656 loss=0.4770\n",
      "Epoch 112 | train_acc=0.8935 loss=0.7776 | test_acc=0.8636 loss=0.4743\n",
      "Epoch 113 | train_acc=0.8959 loss=0.7734 | test_acc=0.8685 loss=0.4675\n",
      "Epoch 114 | train_acc=0.8956 loss=0.7717 | test_acc=0.8685 loss=0.4667\n",
      "Epoch 115 | train_acc=0.8984 loss=0.7679 | test_acc=0.8652 loss=0.4763\n",
      "Epoch 116 | train_acc=0.8993 loss=0.7650 | test_acc=0.8663 loss=0.4665\n",
      "Epoch 117 | train_acc=0.9022 loss=0.7613 | test_acc=0.8691 loss=0.4720\n",
      "Epoch 118 | train_acc=0.9032 loss=0.7576 | test_acc=0.8707 loss=0.4698\n",
      "Epoch 119 | train_acc=0.9011 loss=0.7588 | test_acc=0.8679 loss=0.4720\n",
      "Epoch 120 | train_acc=0.9038 loss=0.7554 | test_acc=0.8668 loss=0.4785\n",
      "Epoch 121 | train_acc=0.9055 loss=0.7516 | test_acc=0.8684 loss=0.4677\n",
      "Epoch 122 | train_acc=0.9037 loss=0.7532 | test_acc=0.8707 loss=0.4659\n",
      "Epoch 123 | train_acc=0.9082 loss=0.7472 | test_acc=0.8687 loss=0.4697\n",
      "Epoch 124 | train_acc=0.9074 loss=0.7482 | test_acc=0.8710 loss=0.4663\n",
      "Epoch 125 | train_acc=0.9104 loss=0.7447 | test_acc=0.8691 loss=0.4791\n",
      "Epoch 126 | train_acc=0.9105 loss=0.7437 | test_acc=0.8705 loss=0.4781\n",
      "Epoch 127 | train_acc=0.9102 loss=0.7444 | test_acc=0.8680 loss=0.4766\n",
      "Epoch 128 | train_acc=0.9104 loss=0.7404 | test_acc=0.8677 loss=0.4756\n",
      "Epoch 129 | train_acc=0.9118 loss=0.7386 | test_acc=0.8696 loss=0.4777\n",
      "Epoch 130 | train_acc=0.9136 loss=0.7346 | test_acc=0.8714 loss=0.4743\n",
      "Epoch 131 | train_acc=0.9120 loss=0.7379 | test_acc=0.8705 loss=0.4683\n",
      "Epoch 132 | train_acc=0.9126 loss=0.7372 | test_acc=0.8730 loss=0.4668\n",
      "Epoch 133 | train_acc=0.9132 loss=0.7362 | test_acc=0.8724 loss=0.4664\n",
      "Epoch 134 | train_acc=0.9154 loss=0.7327 | test_acc=0.8757 loss=0.4653\n",
      "Epoch 135 | train_acc=0.9147 loss=0.7303 | test_acc=0.8735 loss=0.4704\n",
      "Epoch 136 | train_acc=0.9164 loss=0.7295 | test_acc=0.8719 loss=0.4706\n",
      "Epoch 137 | train_acc=0.9193 loss=0.7272 | test_acc=0.8703 loss=0.4766\n",
      "Epoch 138 | train_acc=0.9172 loss=0.7263 | test_acc=0.8705 loss=0.4716\n",
      "Epoch 139 | train_acc=0.9192 loss=0.7242 | test_acc=0.8752 loss=0.4693\n",
      "Epoch 140 | train_acc=0.9171 loss=0.7267 | test_acc=0.8712 loss=0.4740\n",
      "Epoch 141 | train_acc=0.9164 loss=0.7260 | test_acc=0.8715 loss=0.4771\n",
      "Epoch 142 | train_acc=0.9198 loss=0.7210 | test_acc=0.8722 loss=0.4733\n",
      "Epoch 143 | train_acc=0.9208 loss=0.7195 | test_acc=0.8708 loss=0.4750\n",
      "Epoch 144 | train_acc=0.9207 loss=0.7213 | test_acc=0.8721 loss=0.4724\n",
      "Epoch 145 | train_acc=0.9200 loss=0.7212 | test_acc=0.8735 loss=0.4728\n",
      "Epoch 146 | train_acc=0.9197 loss=0.7216 | test_acc=0.8758 loss=0.4654\n",
      "Epoch 147 | train_acc=0.9185 loss=0.7229 | test_acc=0.8760 loss=0.4650\n",
      "Epoch 148 | train_acc=0.9214 loss=0.7204 | test_acc=0.8733 loss=0.4665\n",
      "Epoch 149 | train_acc=0.9208 loss=0.7201 | test_acc=0.8728 loss=0.4698\n",
      "Epoch 150 | train_acc=0.9211 loss=0.7182 | test_acc=0.8739 loss=0.4716\n",
      "Best test acc: 0.876\n"
     ]
    }
   ],
   "source": [
    "# ==== Config + train ====\n",
    "exp_name = \"vit_cifar10_colab_t4\"\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 32,\n",
    "    \"patch_size\": 4,                 # 32/4 -> 8x8 = 64 tokens\n",
    "    \"num_channels\": 3,\n",
    "    \"hidden_size\": 384,              # good capacity on T4\n",
    "    \"num_hidden_layers\": 8,          # if VRAM allows, try 10 later\n",
    "    \"num_attention_heads\": 6,        # 384 / 6 = 64\n",
    "    \"intermediate_size\": 4 * 384,\n",
    "    \"hidden_dropout_prob\": 0.10,     # ↑ regularization\n",
    "    \"attention_probs_dropout_prob\": 0.10,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"num_classes\": 10,\n",
    "    \"qkv_bias\": True,\n",
    "    \"use_faster_attention\": True,\n",
    "    \"drop_path\": 0.20,               # ↑ stochastic depth\n",
    "}\n",
    "# Model\n",
    "model = ViTForClassification(config)\n",
    "\n",
    "# Optimizer / schedule\n",
    "epochs = 150\n",
    "base_lr = 3e-4\n",
    "wd = 5e-2\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=wd)\n",
    "total_steps = epochs * len(train_loader)\n",
    "sched = WarmupCosine(opt, warmup_steps=5*len(train_loader), total_steps=total_steps, min_lr=1e-5)\n",
    "\n",
    "# Loss (label smoothing)\n",
    "loss_fn = LabelSmoothingCE(eps=0.1)\n",
    "\n",
    "trainer = Trainer(model, opt, loss_fn, device=device, scheduler=sched, exp_name=exp_name)\n",
    "history = trainer.fit(train_loader, test_loader, epochs=epochs, save_best=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de57bb7-0a52-4c86-b502-7fe1545b3866",
   "metadata": {},
   "source": [
    "# ==== Optional: reload best checkpoint & eval ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d484a85e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T17:18:34.165513Z",
     "iopub.status.busy": "2025-10-01T17:18:34.165068Z",
     "iopub.status.idle": "2025-10-01T17:18:43.809342Z",
     "shell.execute_reply": "2025-10-01T17:18:43.808186Z"
    },
    "id": "ID4aDSgDhFm7",
    "papermill": {
     "duration": 9.655302,
     "end_time": "2025-10-01T17:18:43.811023",
     "exception": false,
     "start_time": "2025-10-01T17:18:34.155721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded best checkpoint -> test_acc: 0.8760\n"
     ]
    }
   ],
   "source": [
    "# ==== Optional: reload best checkpoint & eval ====\n",
    "best_path = f\"/content/{exp_name}/best.pth\"\n",
    "if os.path.exists(best_path):\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    model.to(device)\n",
    "    test_loss, test_acc = trainer.evaluate(test_loader)\n",
    "    print(\"Reloaded best checkpoint -> test_acc:\", f\"{test_acc:.4f}\")\n",
    "else:\n",
    "    print(\"No best checkpoint found yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02a9a8c",
   "metadata": {
    "papermill": {
     "duration": 0.008479,
     "end_time": "2025-10-01T17:18:44.168242",
     "exception": false,
     "start_time": "2025-10-01T17:18:44.159763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting weightwatcher\n",
      "  Downloading weightwatcher-0.7.5.5-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./env/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch)\n",
      "  Using cached triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting numpy (from weightwatcher)\n",
      "  Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas (from weightwatcher)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m978.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib (from weightwatcher)\n",
      "  Using cached matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: matplotlib-inline in ./env/lib/python3.12/site-packages (from weightwatcher) (0.1.7)\n",
      "Collecting powerlaw (from weightwatcher)\n",
      "  Downloading powerlaw-1.5-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting scikit-learn (from weightwatcher)\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting safetensors (from weightwatcher)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm (from weightwatcher)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->weightwatcher)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->weightwatcher)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->weightwatcher)\n",
      "  Downloading fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib->weightwatcher)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.12/site-packages (from matplotlib->weightwatcher) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib->weightwatcher)\n",
      "  Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->weightwatcher)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./env/lib/python3.12/site-packages (from matplotlib->weightwatcher) (2.9.0.post0)\n",
      "Requirement already satisfied: traitlets in ./env/lib/python3.12/site-packages (from matplotlib-inline->weightwatcher) (5.14.3)\n",
      "Collecting pytz>=2020.1 (from pandas->weightwatcher)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->weightwatcher)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy (from powerlaw->weightwatcher)\n",
      "  Using cached scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->weightwatcher)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->weightwatcher)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->weightwatcher) (1.17.0)\n",
      "Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "Downloading weightwatcher-0.7.5.5-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, nvidia-cusparselt-cu12, mpmath, tzdata, triton, tqdm, threadpoolctl, sympy, safetensors, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, kiwisolver, joblib, fsspec, fonttools, filelock, cycler, scipy, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, torch, powerlaw, weightwatcher\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 filelock-3.19.1 fonttools-4.60.1 fsspec-2025.9.0 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pandas-2.3.3 pillow-11.3.0 powerlaw-1.5 pyparsing-3.2.5 pytz-2025.2 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sympy-1.14.0 threadpoolctl-3.6.0 torch-2.8.0 tqdm-4.67.1 triton-3.4.0 tzdata-2025.2 weightwatcher-0.7.5.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch weightwatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f03420-b96a-4e33-af35-7873e25aeb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11985.100343,
   "end_time": "2025-10-01T17:18:46.687295",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-01T13:59:01.586952",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
